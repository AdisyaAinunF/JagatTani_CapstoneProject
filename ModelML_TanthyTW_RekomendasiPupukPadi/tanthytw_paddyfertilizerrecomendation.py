# -*- coding: utf-8 -*-
"""TanthyTW_PaddyFertilizerRecomendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IMdsMTABdux0_I-XrdF9mT4uukitH_a8

- **Group ID:** [DB8-PG010]
- **Sumber Dataset:** https://drive.google.com/file/d/1By-Y0rcgMuKX8EOLJ790uwJ8ggsVNZ7e/view

## Import Semua Packages/Library yang Digunakan
"""

import os, csv, random, warnings
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns # Import seaborn

# Mencetak versi TensorFlow yang sedang digunakan
print(f"TensorFlow Version: {tf.__version__}")

"""## Data Preparation

### Data Loading
"""

file_id = "1By-Y0rcgMuKX8EOLJ790uwJ8ggsVNZ7e"
url = f"https://drive.google.com/uc?export=download&id={file_id}"
df = pd.read_csv(url, low_memory=False)

df_paddy = df[df['Crop Type'] == 'Paddy'].copy()
print(f"Bentuk data setelah filter 'Paddy': {df_paddy.shape}")

df_paddy.head()

df_paddy = df_paddy.drop(['id', 'Crop Type'], axis=1)
# df_paddy = df_paddy.drop(['id', 'Humidity', 'Moisture', 'Crop Type', 'Nitrogen', 'Potassium'], axis=1)
print("\nKolom yang digunakan untuk model:")
print(df_paddy.columns)

# Visualisasi Matrix Correlation antar Fitur Numerik
# Pilih hanya kolom numerik untuk perhitungan korelasi
df_numeric = df_paddy.select_dtypes(include=np.number)

# Hitung matrix korelasi
correlation_matrix = df_numeric.corr()

# Tampilkan heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Matrix Korelasi antar Fitur Numerik')
plt.show()

df_paddy.head()

df_paddy.describe()

df_paddy.info()

print(f"\nJumlah nilai null per kolom:\n{df_paddy.isnull().sum()}")

print(f"\nJumlah nilai duplicated per kolom:\n{df_paddy.duplicated().sum()}")

"""### Data Preprocessing

#### Pre-Processing
"""

target_column = 'Fertilizer Name'
X = df_paddy.drop(target_column, axis=1)
y = df_paddy[target_column]

# Identifikasi kolom numerik dan kategorikal
numerical_features = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']
# numerical_features = ['Temparature', 'Phosphorous']
categorical_features = ['Soil Type']

# --- Preprocessing Kolom Numerik ---
# Menggunakan StandardScaler, cocok untuk Deep Learning
scaler = StandardScaler()
X_numerical_scaled = scaler.fit_transform(X[numerical_features])
# Ubah kembali ke DataFrame agar mudah digabung
X_numerical_scaled_df = pd.DataFrame(X_numerical_scaled,
                                     columns=numerical_features,
                                     index=X.index)

# --- Preprocessing Kolom Kategorikal ---
# Menggunakan One-Hot Encoding
X_categorical_encoded = pd.get_dummies(X[categorical_features], drop_first=False)

# --- Gabungkan kembali Fitur ---
X_processed = pd.concat([X_numerical_scaled_df, X_categorical_encoded], axis=1)

print(f"\nBentuk data fitur (X) setelah preprocessing: {X_processed.shape}")
print("Contoh 5 baris data fitur (X) yang sudah diproses:")
print(X_processed.head())

# Preprocessing Target (y)
# Menggunakan LabelEncoder untuk mengubah nama pupuk (string) menjadi angka (integer)
label_encoder = LabelEncoder()
y_processed = label_encoder.fit_transform(y)

# Simpan jumlah kelas untuk output layer model
num_classes = len(label_encoder.classes_)
print(f"\nJumlah kelas pupuk (target): {num_classes}")
print(f"Nama kelas: {label_encoder.classes_}")

"""#### Split Dataset"""

# Split Data: 80% Training, 20% Validation
# stratify=y_processed penting agar proporsi kelas pupuk di train dan val set sama
X_train, X_val, y_train, y_val = train_test_split(
    X_processed,
    y_processed,
    test_size=0.2,
    random_state=42,
    stratify=y_processed
)

print(f"\nData Latih (Train): {X_train.shape}, {y_train.shape}")
print(f"Data Validasi (Val): {X_val.shape}, {y_val.shape}")

# Simpan input shape untuk model
input_shape = (X_train.shape[1],)

"""## Modelling"""

# Bangun Arsitektur Model

model = Sequential(name="Fertilizer_Recommendation_Model")
model.add(Input(shape=input_shape, name="Input_Layer"))
model.add(Dense(128, activation='relu', name="Hidden_Layer_1"))
model.add(Dropout(0.4, name="Dropout_1")) # Dropout untuk mengurangi overfitting
model.add(Dense(64, activation='relu', name="Hidden_Layer_2"))
model.add(Dropout(0.2, name="Dropout_2"))
model.add(Dense(num_classes, activation='softmax', name="Output_Layer")) # Softmax untuk klasifikasi multi-kelas

# Compile Model

model.compile(
    optimizer='adam', # Optimizer yang umum dan efektif
    loss='sparse_categorical_crossentropy', # Cocok karena y_train adalah integer (bukan one-hot)
    metrics=['accuracy']
)

# Tampilkan ringkasan model

model.summary()

# Siapkan Callback
# EarlyStopping untuk menghentikan training jika val_loss tidak membaik
# restore_best_weights=True akan mengembalikan bobot model ke epoch terbaik

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15, # Jumlah epoch tanpa perbaikan sebelum berhenti
    restore_best_weights=True,
    verbose=1
)

# Latih Model

print("\nMemulai pelatihan model...")
history = model.fit(
    X_train,
    y_train,
    epochs=100, # Jumlah epoch maksimal
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping],
    verbose=2 # Tampilkan log per epoch
)

print("Pelatihan model selesai.")

"""## Evaluasi dan Visualisasi"""

# Evaluasi Model pada Data Validasi

val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)
print(f"\n--- Hasil Evaluasi Model ---")
print(f"Loss Validasi \t: {val_loss:.4f}")
print(f"Akurasi Validasi: {val_accuracy * 100:.2f}%")

# Visualisasi Akurasi
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Akurasi Training')
plt.plot(history.history['val_accuracy'], label='Akurasi Validasi')
plt.title('Plot Akurasi Model')
plt.xlabel('Epoch')
plt.ylabel('Akurasi')
plt.legend(loc='lower right')

# Visualisasi Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Loss Training')
plt.plot(history.history['val_loss'], label='Loss Validasi')
plt.title('Plot Loss Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.tight_layout()
plt.show()

# Classification Report
# Lakukan prediksi pada data validasi
y_pred_probs = model.predict(X_val)
y_pred_classes = np.argmax(y_pred_probs, axis=1)

# Tampilkan classification report
print("\n--- Classification Report ---")
print(classification_report(
    y_val,
    y_pred_classes,
    target_names=label_encoder.classes_
))

"""## Konversi Model"""

# Inisialisasi TFLite Converter
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Lakukan Konversi
tflite_model = converter.convert()

# Simpan Model TFLite ke File
tflite_model_filename = 'fertilizer_paddy_model.tflite'
with open(tflite_model_filename, 'wb') as f:
    f.write(tflite_model)

print(f"\nModel berhasil dikonversi dan disimpan sebagai '{tflite_model_filename}'")

"""## Inference (Optional)"""

# Muat Model TFLite
interpreter = tf.lite.Interpreter(model_path=tflite_model_filename)
interpreter.allocate_tensors()

# Dapatkan detail input dan output
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(f"\n--- Detail Input Model ---")
print(input_details)
print(f"\n--- Detail Output Model ---")
print(output_details)

# Buat Contoh Data Input (sesuai dengan fitur yang digunakan)
# Contoh ini menggunakan nilai rata-rata dari data training (X_train)
# Anda bisa mengganti nilai ini dengan data baru yang ingin diprediksi
sample_input_data = {
    'Temparature': [X_train['Temparature'].mean()],
    'Humidity': [X_train['Humidity'].mean()],
    'Moisture': [X_train['Moisture'].mean()],
    'Nitrogen': [X_train['Nitrogen'].mean()],
    'Potassium': [X_train['Potassium'].mean()],
    'Phosphorous': [X_train['Phosphorous'].mean()],
    'Soil Type_Black': [X_train['Soil Type_Black'].mean()],
    'Soil Type_Clayey': [X_train['Soil Type_Clayey'].mean()],
    'Soil Type_Loamy': [X_train['Soil Type_Loamy'].mean()],
    'Soil Type_Red': [X_train['Soil Type_Red'].mean()],
    'Soil Type_Sandy': [X_train['Soil Type_Sandy'].mean()],
}

sample_df = pd.DataFrame(sample_input_data)

print("\nContoh data input:")
print(sample_df)

# Preprocessing Contoh Data Input
# Pastikan urutan kolom sesuai dengan urutan saat training
# Gunakan scaler dan one-hot encoder yang sama seperti saat training
# (Dalam contoh ini, karena menggunakan mean dari data yang sudah diskalakan/encoded,
# langkah scaling/encoding ulang tidak diperlukan untuk data rata-rata.
# Namun, untuk data baru, Anda perlu menggunakan scaler dan encoder yang sudah fit
# pada data training).
# Untuk data baru:
# sample_numerical_scaled = scaler.transform(sample_df[numerical_features])
# sample_categorical_encoded = pd.get_dummies(sample_df[categorical_features], drop_first=False)
# sample_processed = pd.concat([pd.DataFrame(sample_numerical_scaled, columns=numerical_features), sample_categorical_encoded], axis=1)

# Karena kita menggunakan mean dari data yang sudah diproses, kita bisa langsung menggunakan sample_df
sample_processed = sample_df.astype(np.float32) # TFLite biasanya butuh float32

# Pastikan urutan kolom sample_processed sama dengan X_processed
sample_processed = sample_processed[X_processed.columns]


print("\nContoh data input setelah preprocessing:")
print(sample_processed)


# Lakukan Inferensi
# Ubah data input ke format tensor TFLite
input_tensor = tf.constant(sample_processed.values, dtype=tf.float32)
interpreter.set_tensor(input_details[0]['index'], input_tensor)

# Jalankan inferensi
interpreter.invoke()

# Ambil hasil output
output_tensor = interpreter.get_tensor(output_details[0]['index'])

# Hasil output adalah probabilitas untuk setiap kelas
predicted_probabilities = output_tensor[0]
predicted_class_index = np.argmax(predicted_probabilities)
predicted_fertilizer = label_encoder.classes_[predicted_class_index]

print(f"\nHasil Inferensi:")
print(f"Probabilitas Prediksi: {predicted_probabilities}")
print(f"Indeks Kelas Prediksi: {predicted_class_index}")
print(f"Nama Pupuk Prediksi  : {predicted_fertilizer}")

"""Membuat File README.md"""

# Membuat file README.md
readme_content = """
# Proyek Rekomendasi Pupuk Padi

Ini adalah proyek model Machine Learning (Deep Learning) untuk merekomendasikan jenis pupuk yang sesuai untuk tanaman padi berdasarkan berbagai parameter tanah dan lingkungan.

## Dataset
Dataset yang digunakan berisi data parameter seperti Suhu, Kelembaban, Kelembaban Tanah, Jenis Tanah, Nitrogen, Kalium, Fosfor, dan Jenis Pupuk yang digunakan untuk tanaman padi.

## Data Preprocessing
Data dilakukan preprocessing meliputi:
- Filtering data hanya untuk 'Crop Type' = 'Paddy'.
- Menghapus kolom yang tidak relevan ('id', 'Crop Type').
- Scaling fitur numerik menggunakan `StandardScaler`.
- One-Hot Encoding fitur kategorikal ('Soil Type').
- Label Encoding target ('Fertilizer Name').
- Split data menjadi Training (80%) dan Validation (20%) dengan stratifikasi.

## Arsitektur Model
Model menggunakan arsitektur Sequential Deep Learning dengan layer:
- Input Layer (sesuai jumlah fitur setelah preprocessing)
- Dense Layer (128 unit, aktivasi 'relu', dengan Dropout 0.4)
- Dense Layer (64 unit, aktivasi 'relu', dengan Dropout 0.2)
- Output Layer (sesuai jumlah kelas pupuk, aktivasi 'softmax')

Model dikompilasi menggunakan optimizer 'adam' dan loss 'sparse_categorical_crossentropy'. Digunakan callback `EarlyStopping` dengan `patience=15` dan `restore_best_weights=True`.

## Hasil Pelatihan dan Evaluasi
(Bagian ini dapat diisi manual setelah menjalankan notebook)
*   **Jumlah Epoch Terbaik:** (Akan diisi)
*   **Loss Training (Epoch Terbaik):** (Akan diisi)
*   **Akurasi Training (Epoch Terbaik):** (Akan diisi)
*   **Loss Validasi (Epoch Terbaik):** {val_loss:.4f}
*   **Akurasi Validasi:** {val_accuracy_percent:.2f}%
*   **Classification Report:** (Akan dilampirkan atau dijelaskan secara manual)

## File Model yang Dihasilkan
Proyek ini menghasilkan model dalam format TF-Lite:
*   **TF-Lite:** `fertilizer_paddy_model.tflite`

## Cara Penggunaan (Inferensi)
(Bagian ini dapat menjelaskan cara memuat model TFLite dan melakukan prediksi menggunakan data input baru)
"""

try:
    # Format string dengan hasil evaluasi dari sel sebelumnya
    val_accuracy_percent = val_accuracy * 100
    readme_content = readme_content.format(val_loss=val_loss, val_accuracy_percent=val_accuracy_percent)
    with open('README.md', 'w') as f:
        f.write(readme_content)
    print("File README.md berhasil diperbarui.")
except Exception as e:
    print(f"Terjadi error saat memperbarui file README.md: {e}")

"""Membuat File Requirements"""

# Creating requirements.txt file
!pip freeze > requirements.txt